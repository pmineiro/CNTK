########################################################################
# DSSM with Residual GRU 
# 
# no attention
# no tying of query and context GRU parameters (but embedding is tied)
#
########################################################################

# Finished Epoch[36 of 200]: [Training] ce = 0.61686070 * 100000; errs = 35.146% * 100000; totalSamplesSeen = 3600000; learningRatePerSample = 0.00039062501; epoc hTime=3336.1s
# Finished Epoch[36 of 200]: [Validate] ce = 0.81833946 * 10000; errs = 50.000% * 10000

RunRootDir = ".."      # default if not overridden
DataDir    = "$RunRootDir$/Data"
OutDir     = "$RunRootDir$/Out"

command = train

deviceId = auto 
ExpId = ubuntu-1

modelPath  = "$OutDir$/$ExpId$/Ubuntu.dnn"
stderr     = "$OutDir$/$ExpId$/Ubuntu"

vocabDim = 121223
hiddenDim = 320
embeddingDim = 320
maxLayer = 1

globalInput = [ Context = [ dim = "$vocabDim$"; format = "sparse" ]; Query = [ dim = "$vocabDim$"; format = "sparse" ]; Label = [ dim = 1; format = "dense" ]; MultiLabel = [ dim = 2; format = "dense" ] ]

#---------------------------
# network
#---------------------------

BrainScriptNetworkBuilder = (new ComputationNetwork [
  vocabDim = $vocabDim$
  hiddenDim = $hiddenDim$
  embeddingDim = $embeddingDim$
  precision = "float"
  maxLayer = $maxLayer$
  
  netDims[i:0..maxLayer] = hiddenDim

  contextAxis = DynamicAxis ();
  Context = Input (vocabDim, dynamicAxis=contextAxis, tag='feature')
  queryAxis = DynamicAxis ()
  Query = Input (vocabDim, dynamicAxis=queryAxis, tag='feature')
  Label = Input (1, tag='label')
  MultiLabel = Input (2, tag='label')

  # "averaging" network

  avgNet (in) = [
    mysum = in + PastValue (0, mysum, defaultHiddenActivation=0)
    mycount = BS.Constants.OnesLike (in) + PastValue (0, mycount, defaultHiddenActivation=0)
    myavg = ElementDivide (mysum, mycount)
  ].myavg

  # embedding
  
  Einput = BS.Parameter ($vocabDim$, embeddingDim, init='gaussian')

  # context GRU

  contextEmbedded = TransposeTimes (Einput, Context)
  contextNet = BS.RNNs.RecurrentBiresidualGRUStack (netDims, contextEmbedded, inputDim=embeddingDim, addDropout=true)
  contextOutput = Dropout (contextNet[Length (netDims) - 1].h)
  avgContextNetOutput = avgNet (contextOutput)
  lastAvgContextNetOutput = BS.Sequences.Last (avgContextNetOutput)

  # query GRU

  queryEmbedded = TransposeTimes (Einput, Query)
  queryNet = BS.RNNs.RecurrentBiresidualGRUStack (netDims, queryEmbedded, inputDim=embeddingDim, addDropout=true)
  queryOutput = Dropout (queryNet[Length (netDims) - 1].h)
  avgQueryNetOutput = avgNet (queryOutput)
  preLastAvgQueryNetOutput = BS.Sequences.Last (avgQueryNetOutput)
  lastAvgQueryNetOutput = ReconcileDynamicAxis (preLastAvgQueryNetOutput, lastAvgContextNetOutput)

  # inner product similarity

  simBias = BS.ParameterTensor ((1), init='fixedValue', value=-1)
  similarityQuad = simBias + TransposeTimes (lastAvgContextNetOutput, lastAvgQueryNetOutput)

  # loss function
  
  comboScore = Splice ( ( similarityQuad : BS.Constants.ZeroesLike (similarityQuad) ), axis=1)
  recCombo = ReconcileDynamicAxis (comboScore, MultiLabel)
  ce = CrossEntropyWithSoftmax (MultiLabel, recCombo, tag='criterion')
  errs = ErrorPrediction (MultiLabel, recCombo, tag='evaluation')

#  p = Sigmoid (similarityQuad)
#  recP = ReconcileDynamicAxis (p, Label)
#  lr = Logistic (Label, recP, tag='evaluation')
#  errs = SquareError (Label, recP, tag='evaluation')
])

train = [
  action = "train"
  traceLevel = 1
  epochSize = 0

  reader = [
    readerType = "CNTKTextFormatReader"
    file = "$DataDir$/shortubuntu.train"
    randomize = "true"

    input = $globalInput$
  ]

  cvReader = [
    readerType = "CNTKTextFormatReader"
    file = "$DataDir$/shortubuntu.valid"
    randomize = "false"

    input = $globalInput$
  ]

  SGD = [ 
    modelPath = "$modelPath$"
    epochSize = 0
    keepCheckPointFiles = "false"
    maxEpochs = 200
    minibatchSize = 512
    learningRatesPerMB = 2:1*3:0.2
    momentumAsTimeConstant = 1024
    dropoutRate = 0.0
    gradUpdateType = "RMSProp"

#    # settings for Auto Adjust Learning Rate
#    AutoAdjust = [
#        autoAdjustLR = "adjustAfterEpoch"
#        reduceLearnRateIfImproveLessThan = -1
#        continueReduce = false
#        increaseLearnRateIfImproveMoreThan = 1000000000
#        learnRateDecreaseFactor = 0.5
#        learnRateIncreaseFactor = 1.382
#    ]
  ]
]

# TODO: this can only test accuracy with 1 distractor
test = [
  action = "eval"

  minibatchSize = 1024
  traceLevel = 1
  epochSize = 0

  reader = [
    readerType = "CNTKTextFormatReader"
    file = "$DataDir$/shortubuntu.test"
    randomize = "false"

    input = $globalInput$
  ]
]
