########################################################################
# trigram variant, with attention 
########################################################################

RunRootDir = ".."      # default if not overridden
DataDir    = "$RunRootDir$/Data"
OutDir     = "$RunRootDir$/Out"

command = train

deviceId = auto 
ExpId = ubuntu-3

modelPath  = "$OutDir$/$ExpId$/Ubuntu.dnn"
stderr     = "$OutDir$/$ExpId$/Ubuntu"

fileext = "tri" # or "quad"
vocabDim = 15915 # or 16468
hiddenDim = 320
embeddingDim = 320
maxLayer = 1
initLearningRate = 0.5
dropoutRate = 0.0
minibatchSize = 768
momentum = 0.9
restartFrom = "/dev/null"

globalInput = [ Context = [ dim = "$vocabDim$"; format = "sparse" ]; Query1 = [ dim = "$vocabDim$"; format = "sparse" ]; Query2 = [ dim = "$vocabDim$"; format = "sparse" ]; MultiLabel = [ dim = 2; format = "dense" ] ]

#---------------------------
# network
#---------------------------

BrainScriptNetworkBuilder = (if "$restartFrom$" == "/dev/null"
then (new ComputationNetwork [

include "grufactory.bs"

#---------------------------
# definition
#---------------------------

  vocabDim = $vocabDim$
  hiddenDim = $hiddenDim$
  embeddingDim = $embeddingDim$
  precision = "float"
  maxLayer = $maxLayer$
  
  netDims[i:0..maxLayer] = hiddenDim

  contextAxis = DynamicAxis ();
  Context = Input (vocabDim, dynamicAxis=contextAxis, tag='feature')
  queryAxis1 = DynamicAxis ()
  Query1 = Input (vocabDim, dynamicAxis=queryAxis1, tag='feature')
  queryAxis2 = DynamicAxis ()
  Query2 = Input (vocabDim, dynamicAxis=queryAxis2, tag='feature')
  MultiLabel = Input (2, tag='label')

  # average (over dynamic axis) network

  AverageOverSequence (in) = [
    mysum = in + PastValue (0, mysum, defaultHiddenActivation=0)
    mycount = BS.Constants.OnesLike (in) + PastValue (0, mycount, defaultHiddenActivation=0)
    myavg = ElementDivide (mysum, mycount)
    out = BS.Sequences.Last (myavg)
  ].out

  # softmax (over dynamic axis) network
  SoftMaxOverSequence (z) = [
    runningLogSum = BS.LogPlus (z, PastValue (0, runningLogSum, defaultHiddenActivation=-1e30))
    logSum = BS.Boolean.If (BS.Loop.IsLast (runningLogSum),    # if last entry
                   /*then*/ runningLogSum,                     # then copy that
                   /*else*/ FutureValue (0, logSum))           # else just propagate to the front
    result = Exp (z - logSum)
  ].result

  # softmax weighted average (over dynamic axis) network

  SoftMaxAverageOverSequence (attW, z) = [
    attPre = FlattenDimensions (TransposeTimes (attW, z), 1, 2)
    att = SoftMaxOverSequence (attPre)
    attZ = ElementTimes (att, z)
    mysum = attZ + PastValue (0, mysum, defaultHiddenActivation=0)
    out = BS.Sequences.Last (mysum)
  ].out

  # tied parameters
  Einput = BS.Parameter (vocabDim, embeddingDim, init='gaussian')
  Factory = RecurrentBiresidualGRUStackFactory (netDims, embeddingDim)
  attV = Parameter (hiddenDim, 1, init='fixedValue', value=0.0)

  # context GRU
  contextEmbedded = TransposeTimes (Einput, Context)
  contextNet = RecurrentBiresidualGRUStackInstantiate (Factory, contextEmbedded, addDropout=true)
  contextOutput = contextNet[Length (netDims) - 1].h
  lastAvgContextNetOutput = SoftMaxAverageOverSequence (attV, contextOutput)

  # query GRU

  queryEmbedded1 = TransposeTimes (Einput, Query1)
  queryNet1 = RecurrentBiresidualGRUStackInstantiate (Factory, queryEmbedded1, addDropout=true)
  queryOutput1 = queryNet1[Length (netDims) - 1].h
  lastAvgQueryNetOutput1 = SoftMaxAverageOverSequence (attV, queryOutput1)

  queryEmbedded2 = TransposeTimes (Einput, Query2)
  queryNet2 = RecurrentBiresidualGRUStackInstantiate (Factory, queryEmbedded2, addDropout=true)
  queryOutput2 = queryNet2[Length (netDims) - 1].h
  lastAvgQueryNetOutput2 = SoftMaxAverageOverSequence (attV, queryOutput2)

  # inner product similarity

  M = BS.Parameter (hiddenDim, hiddenDim, init='identity')
  similarityQuad1 = TransposeTimes (ReconcileDynamicAxis (lastAvgContextNetOutput, lastAvgQueryNetOutput1), TransposeTimes (M, lastAvgQueryNetOutput1))
  similarityQuad2 = TransposeTimes (ReconcileDynamicAxis (lastAvgContextNetOutput, lastAvgQueryNetOutput2), TransposeTimes (M, lastAvgQueryNetOutput2))

  # loss function
  
  comboScore = FlattenDimensions (Splice ( ( similarityQuad1 : ReconcileDynamicAxis (similarityQuad2, similarityQuad1 ) ), axis=1 ), 1, 2)
  recCombo = ReconcileDynamicAxis (comboScore, MultiLabel)
  ce = CrossEntropyWithSoftmax (MultiLabel, recCombo, tag='criterion')
  errs = ErrorPrediction (MultiLabel, recCombo, tag='evaluation')
])
else
(BS.Network.Load ("$restartFrom$"))
)

train = [
  action = "train"
  traceLevel = 1
  epochSize = 0

  reader = [
    readerType = "CNTKTextFormatReader"
    file = "$DataDir$/shortubuntu3$fileext$.megatrain"
    randomize = "true"

    input = $globalInput$
  ]

  cvReader = [
    readerType = "CNTKTextFormatReader"
    file = "$DataDir$/shortubuntu3$fileext$.valid"
    randomize = "false"

    input = $globalInput$
  ]

  SGD = [ 
    modelPath = "$modelPath$"
    epochSize = 0
    keepCheckPointFiles = "false"
    maxEpochs = 200
    minibatchSize = $minibatchSize$
    learningRatesPerMB = $initLearningRate$
    momentumPerMB = $momentum$
    dropoutRate = $dropoutRate$
    gradUpdateType = "RMSProp"

    # settings for Auto Adjust Learning Rate
    AutoAdjust = [
        autoAdjustLR = "adjustAfterEpoch"
        loadBestModel = "true"
        useEvalCriterionControlLR = "true"
    ]
  ]
]

# TODO: this can only test accuracy with 1 distractor
test = [
  action = "eval"

  minibatchSize = $minibatchSize$
  traceLevel = 1
  epochSize = 0

  reader = [
    readerType = "CNTKTextFormatReader"
    file = "$DataDir$/shortubuntu3$fileext$.test"
    randomize = "false"

    input = $globalInput$
  ]
]
